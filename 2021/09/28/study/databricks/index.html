<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Databricks | wobushitiegan</title><meta name="keywords" content="博客，读书笔记，学习笔记，个人分享，复盘，总结，反思"><meta name="author" content="我不是铁杆"><meta name="copyright" content="我不是铁杆"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="直接上官方文档： Azure Databricks 文档 | Microsoft Docs   Databricks包含三大模块：  Databricks 数据科学与工程 Databricks机器学习 Databricks SQL（预览版）  这里我们目前学习的是Databricks数据科学与工程， 主要学习里边的Delta Lake 和 Delta Engine   Delta Lake 和 D">
<meta property="og:type" content="article">
<meta property="og:title" content="Databricks">
<meta property="og:url" content="https://wobushitiegan.github.io/2021/09/28/study/databricks/index.html">
<meta property="og:site_name" content="wobushitiegan">
<meta property="og:description" content="直接上官方文档： Azure Databricks 文档 | Microsoft Docs   Databricks包含三大模块：  Databricks 数据科学与工程 Databricks机器学习 Databricks SQL（预览版）  这里我们目前学习的是Databricks数据科学与工程， 主要学习里边的Delta Lake 和 Delta Engine   Delta Lake 和 D">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png">
<meta property="article:published_time" content="2021-09-28T11:11:06.456Z">
<meta property="article:modified_time" content="2021-09-28T12:03:17.502Z">
<meta property="article:author" content="我不是铁杆">
<meta property="article:tag" content="博客，读书笔记，学习笔记，个人分享，复盘，总结，反思">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://wobushitiegan.github.io/2021/09/28/study/databricks/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Databricks',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-09-28 20:03:17'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1guwp9xw09sj60b00b2q3u02.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">4</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">wobushitiegan</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Databricks</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-28T11:11:06.456Z" title="发表于 2021-09-28 19:11:06">2021-09-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-09-28T12:03:17.502Z" title="更新于 2021-09-28 20:03:17">2021-09-28</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Databricks"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2021/09/28/study/databricks/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2021/09/28/study/databricks/" itemprop="commentCount"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>直接上官方文档： <a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/">Azure Databricks 文档 | Microsoft Docs</a></p>
<blockquote>
<p> Databricks包含三大模块：</p>
<ol>
<li>Databricks 数据科学与工程</li>
<li>Databricks机器学习</li>
<li>Databricks SQL（预览版）</li>
</ol>
<p>这里我们目前学习的是Databricks数据科学与工程， 主要学习里边的Delta Lake 和 Delta Engine</p>
</blockquote>
<blockquote>
<p><strong>Delta Lake 和 Delta Engine指南</strong></p>
<p><strong>Delta Lake</strong> 是可以提高 Data Lake 可靠性的开源存储层。 Delta Lake 提供 ACID 事务和可缩放的元数据处理，并可以统一流处理和批数据处理。 Delta Lake 在现有 Data Lake 的顶层运行，与 Apache Spark API 完全兼容。 利用 Azure Databricks 上的 Delta Lake，便可以根据工作负载模式配置 Delta Lake。</p>
<p>Azure Databricks 还包括 <strong>Delta Engine</strong>，这为快速交互式查询提供了优化的布局和索引。</p>
</blockquote>
<p><strong>写在前边</strong></p>
<p>1、因为这个技术是Azure新推出的，网上的推文或者解决问题的博客会比较少，这里自己记录一下项目开发过程中遇到的问题及解决方法，来当一个开拓者。</p>
<p>2、要想使用好这个API，得先利其器，那就把官网的文档先给过一遍。其中的新特性及API心里先有个大概印象，让我有机会去犯错，而不是不知道有这个功能。</p>
<p>3、通过读当前微软的Azure的文档，就是上边的官方文档，让我很清楚的知道了规整文字的力量，让我一步步的把这个技术看懂，看会。可能大部分的官方文档都是这样的，文档的目的就是为了能够让新手看懂。所以我们的个人笔记以及平时的工作汇报等各种方式的记录，都可以有理有据的有条不紊的进行记录和整理。</p>
<p>4、因为我目前用不到Python语言来进行这个项目的开发，所以这里的案例，我只用的SQL和Scala两种语言来进行记录此笔记。 </p>
<h2 id="一、Delta-Lake-快速入门"><a href="#一、Delta-Lake-快速入门" class="headerlink" title="一、Delta Lake 快速入门"></a>一、Delta Lake 快速入门</h2><h3 id="1、创建表"><a href="#1、创建表" class="headerlink" title="1、创建表"></a><strong>1、创建表</strong></h3><blockquote>
<p>若要创建 Delta 表，可以使用现有的 Apache Spark SQL 代码，并将格式从 <code>parquet</code>、<code>csv</code>、<code>json</code> 等更改为 <code>delta</code>。</p>
<p>对于所有文件类型，都需要将文件读入数据帧并以 <code>delta</code> 格式写出</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SQL</span>形式： </span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> events</span><br><span class="line"><span class="keyword">USING</span> delta</span><br><span class="line"><span class="keyword">AS</span> <span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> json.`<span class="operator">/</span>data<span class="operator">/</span>events<span class="operator">/</span>`</span><br><span class="line"><span class="comment">---------------------------------------------------------------------------------------------------------------------</span></span><br><span class="line">spark： </span><br><span class="line">events <span class="operator">=</span> spark.read.json(&quot;/databricks-datasets/structured-streaming/events/&quot;)</span><br><span class="line">events.write.format(&quot;delta&quot;).save(&quot;/mnt/delta/events&quot;)</span><br><span class="line">spark.sql(&quot;CREATE TABLE events USING DELTA LOCATION &#x27;/mnt/delta/events/&#x27;&quot;)</span><br></pre></td></tr></table></figure>

<h3 id="2、将数据分区"><a href="#2、将数据分区" class="headerlink" title="2、将数据分区"></a>2、将数据分区</h3><blockquote>
<p>若要加速其谓词涉及分区列的查询，可以对数据进行分区。</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SQL</span>形式：</span><br><span class="line">若要在使用 <span class="keyword">SQL</span> 创建 Delta 表时对数据进行分区，请指定 PARTITIONED <span class="keyword">BY</span> 列。</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> events (</span><br><span class="line">  <span class="type">date</span> <span class="type">DATE</span>,</span><br><span class="line">  eventId STRING,</span><br><span class="line">  eventType STRING,</span><br><span class="line">  data STRING)</span><br><span class="line"><span class="keyword">USING</span> delta</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (<span class="type">date</span>)</span><br><span class="line"><span class="comment">---------------------------------------------------------------------------------------------------------------------</span></span><br><span class="line">Spark形式： </span><br><span class="line">events <span class="operator">=</span> spark.read.json(&quot;/databricks-datasets/structured-streaming/events/&quot;)</span><br><span class="line">events.write.partitionBy(&quot;date&quot;).format(&quot;delta&quot;).save(&quot;/mnt/delta/events&quot;)</span><br><span class="line">spark.sql(&quot;CREATE TABLE events USING DELTA LOCATION &#x27;/mnt/delta/events/&#x27;&quot;)</span><br></pre></td></tr></table></figure>

<h3 id="3、修改表"><a href="#3、修改表" class="headerlink" title="3、修改表"></a>3、修改表</h3><blockquote>
<p>Delta Lake 支持使用一组丰富的操作来修改表。</p>
</blockquote>
<ul>
<li><strong>流式处理到表的写入</strong><br>你可以使用结构化流式处理将数据写入 Delta 表。 即使有针对表并行运行的其他流或批处理查询，Delta Lake 事务日志也可确保仅处理一次。 默认情况下，流在追加模式下运行，这会将新记录添加到表中。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Python</span> <span class="type">Spark</span> ：</span><br><span class="line"></span><br><span class="line">from pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">inputPath = <span class="string">&quot;/databricks-datasets/structured-streaming/events/&quot;</span></span><br><span class="line"></span><br><span class="line">jsonSchema = <span class="type">StructType</span>([ <span class="type">StructField</span>(<span class="string">&quot;time&quot;</span>, <span class="type">TimestampType</span>(), <span class="type">True</span>), <span class="type">StructField</span>(<span class="string">&quot;action&quot;</span>, <span class="type">StringType</span>(), <span class="type">True</span>) ])</span><br><span class="line"></span><br><span class="line">eventsDF = (</span><br><span class="line">  spark</span><br><span class="line">    .readStream</span><br><span class="line">    .schema(jsonSchema) # <span class="type">Set</span> the schema of the <span class="type">JSON</span> data</span><br><span class="line">    .option(<span class="string">&quot;maxFilesPerTrigger&quot;</span>, <span class="number">1</span>) # <span class="type">Treat</span> a sequence of files as a stream by picking one file at a time</span><br><span class="line">    .json(inputPath)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">(eventsDF.writeStream</span><br><span class="line">  .outputMode(<span class="string">&quot;append&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;checkpointLocation&quot;</span>, <span class="string">&quot;/mnt/delta/events/_checkpoints/etl-from-json&quot;</span>)</span><br><span class="line">  .table(<span class="string">&quot;events&quot;</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>批量upsert</strong></li>
</ul>
<p>若要将一组更新和插入合并到现有表中，请使用 <code>MERGE INTO</code> 语句。 例如，下面的语句将获取一个更新流，并将其合并到 <code>events</code> 表中。 如果已存在具有相同 <code>eventId</code> 的事件，Delta Lake 会使用给定的表达式更新数据列。 如果没有匹配的事件，Delta Lake 会添加一个新行。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">MERGE</span> <span class="keyword">INTO</span> events</span><br><span class="line"><span class="keyword">USING</span> updates</span><br><span class="line"><span class="keyword">ON</span> events.eventId <span class="operator">=</span> updates.eventId</span><br><span class="line"><span class="keyword">WHEN</span> MATCHED <span class="keyword">THEN</span></span><br><span class="line">  UPDATE <span class="keyword">SET</span></span><br><span class="line">    events.data <span class="operator">=</span> updates.data</span><br><span class="line"><span class="keyword">WHEN</span> <span class="keyword">NOT</span> MATCHED</span><br><span class="line">  <span class="keyword">THEN</span> <span class="keyword">INSERT</span> (<span class="type">date</span>, eventId, data) <span class="keyword">VALUES</span> (<span class="type">date</span>, eventId, data)</span><br></pre></td></tr></table></figure>

<p>执行 <code>INSERT</code> 时必须为表中的每个列指定一个值（例如，当现有数据集中没有匹配行时，必须这样做）。 但是，你不需要更新所有值。</p>
<h3 id="4、读取表"><a href="#4、读取表" class="headerlink" title="4、读取表"></a>4、读取表</h3><blockquote>
<p>可以通过指定 DBFS 上的路径 (<code>&quot;/mnt/delta/events&quot;</code>) 或表名 (<code>&quot;events&quot;</code>) 来访问 Delta 表中的数据：</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spark： </span><br><span class="line"><span class="keyword">val</span> events = spark.read.format(<span class="string">&quot;delta&quot;</span>).load(<span class="string">&quot;/mnt/delta/events&quot;</span>)</span><br><span class="line">或者</span><br><span class="line">events = spark.table(<span class="string">&quot;events&quot;</span>)</span><br><span class="line">---------------------------------------------------------------------------------------------------------------------</span><br><span class="line"><span class="type">SQL</span>： </span><br><span class="line"><span class="type">SELECT</span> * <span class="type">FROM</span> delta.`/mnt/delta/events`</span><br><span class="line">或</span><br><span class="line"><span class="type">SELECT</span> * <span class="type">FROM</span> events</span><br></pre></td></tr></table></figure>

<h3 id="5、显示表历史记录"><a href="#5、显示表历史记录" class="headerlink" title="5、显示表历史记录"></a>5、显示表历史记录</h3><p>若要查看表的历史记录，请使用 <code>DESCRIBE HISTORY</code> 语句，该语句提供对表进行的每次写入的出处信息，包括表版本、操作、用户等。 请参阅<a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/delta-utility#delta-history">检索 Delta 表历史记录</a>。</p>
<h3 id="6、查询较早版本的表（按时间顺序查看）"><a href="#6、查询较早版本的表（按时间顺序查看）" class="headerlink" title="6、查询较早版本的表（按时间顺序查看）"></a>6、查询较早版本的表（按时间顺序查看）</h3><p>Delta Lake 按时间顺序查看允许你查询 Delta 表的旧快照。</p>
<p>对于 <code>timestamp_string</code>，只接受日期或时间戳字符串。 例如，<code>&quot;2019-01-01&quot;</code> 和 <code>&quot;2019-01-01&#39;T&#39;00:00:00.000Z&quot;</code>。</p>
<p>若要查询较早版本的表，请在 <code>SELECT</code> 语句中指定版本或时间戳。 例如，若要从上述历史记录中查询版本 0，请使用：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> events VERSION <span class="keyword">AS</span> <span class="keyword">OF</span> <span class="number">0</span></span><br><span class="line">或 <span class="comment">---------------------------------------------------------------------------------------------------------------------</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> events <span class="type">TIMESTAMP</span> <span class="keyword">AS</span> <span class="keyword">OF</span> <span class="string">&#x27;2019-01-29 00:37:58&#x27;</span></span><br><span class="line"></span><br><span class="line">备注：</span><br><span class="line">由于版本 <span class="number">1</span> 位于时间戳 <span class="string">&#x27;2019-01-29 00:38:10&#x27;</span> 处，因此，若要查询版本 <span class="number">0</span>，可以使用范围 <span class="string">&#x27;2019-01-29 00:37:58&#x27;</span> 到 <span class="string">&#x27;2019-01-29 00:38:09&#x27;</span>（含）中的任何时间戳。</span><br></pre></td></tr></table></figure>

<h3 id="7、优化表"><a href="#7、优化表" class="headerlink" title="7、优化表"></a>7、优化表</h3><p>对表执行多个更改后，可能会有很多小文件。 为了提高读取查询的速度，你可以使用 <code>OPTIMIZE</code> 将小文件折叠为较大的文件：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">OPTIMIZE delta.`<span class="operator">/</span>mnt<span class="operator">/</span>delta<span class="operator">/</span>events`</span><br><span class="line">或</span><br><span class="line">OPTIMIZE events</span><br></pre></td></tr></table></figure>

<h3 id="8、按列进行Z排序"><a href="#8、按列进行Z排序" class="headerlink" title="8、按列进行Z排序"></a>8、按列进行Z排序</h3><p>为了进一步提高读取性能，你可以通过 Z 排序将相关的信息放置在同一组文件中。 Delta Lake 数据跳过算法会自动使用此并置，大幅减少需要读取的数据量。 若要对数据进行 Z 排序，请在 <code>ZORDER BY</code> 子句中指定要排序的列。 例如，若要按 <code>eventType</code> 并置，请运行：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">OPTIMIZE events</span><br><span class="line">  ZORDER <span class="keyword">BY</span> (eventType)</span><br></pre></td></tr></table></figure>

<p>有关运行 <code>OPTIMIZE</code> 时可用的完整选项集，请参阅<a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/optimizations/file-mgmt#delta-optimize">压缩（装箱）</a>。</p>
<h3 id="9、介绍性笔记本（Demo）"><a href="#9、介绍性笔记本（Demo）" class="headerlink" title="9、介绍性笔记本（Demo）"></a>9、介绍性笔记本（Demo）</h3><blockquote>
<p>这里提供Scala 和 SQL的两个demo。 看了一遍之后，对上一章节的入门操作就有了大概的了解了。good，建议学习。</p>
<p>这些笔记本展示了如何将 JSON 数据转换为 Delta Lake 格式，创建 Delta 表，追加到表，优化生成的表，最后使用 Delta Lake 元数据命令显示表历史记录、格式和详细信息。</p>
<p>若要试用 Delta Lake，请参阅<a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/azure-databricks/quickstart-create-databricks-workspace-portal">快速入门：使用 Azure 门户在 Azure Databricks 上运行 Spark 作业</a>。</p>
</blockquote>
<p><strong>Delta Lake 快速入门 Scala 笔记本</strong> <a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/_static/notebooks/delta/quickstart-scala.html">获取笔记本</a></p>
<p><strong>Delta Lake 快速入门 SQL 笔记本</strong>  <a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/_static/notebooks/delta/quickstart-sql.html">获取笔记本</a></p>
<h2 id="二、数据的读取与写入"><a href="#二、数据的读取与写入" class="headerlink" title="二、数据的读取与写入"></a>二、数据的读取与写入</h2><h3 id="1、将数据引入Delta-Lake"><a href="#1、将数据引入Delta-Lake" class="headerlink" title="1、将数据引入Delta Lake"></a>1、将数据引入Delta Lake</h3><blockquote>
<p>Azure Databricks 提供多种方法来帮助你讲数据引入Delta Lake </p>
<ul>
<li>合作伙伴集成</li>
<li>COPY INTO SQL 命令</li>
<li>自动加载程序</li>
</ul>
<p>上边这些，是直接提供的服务，如果能用到的话，再来看官网。 </p>
<p> <a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/delta-ingest">将数据引入到 Delta Lake - Azure Databricks - Workspace | Microsoft Docs</a></p>
</blockquote>
<h3 id="2、表批量读取和写入"><a href="#2、表批量读取和写入" class="headerlink" title="2、表批量读取和写入"></a>2、表批量读取和写入</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/delta-batch">表批量读取和写入 - Azure Databricks - Workspace | Microsoft Docs</a></p>
<p>备注上官网文档路径。</p>
</blockquote>
<h4 id="2-1、-创建表"><a href="#2-1、-创建表" class="headerlink" title="2.1、 创建表"></a>2.1、 创建表</h4><blockquote>
<p>Delta Lake 支持创建两种类型的表：元存储中定义的表和由路径定义的表。</p>
</blockquote>
<h5 id="2-1-1、创建表的三种方式。"><a href="#2-1-1、创建表的三种方式。" class="headerlink" title="2.1.1、创建表的三种方式。"></a>2.1.1、创建表的三种方式。</h5><ul>
<li><p>SQL DDL命令：可以使用 Apache Spark 支持的标准 SQL DDL 命令（例如 CREATE TABLE 和 REPLACE TABLE）来创建 Delta 表。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> events (</span><br><span class="line">  <span class="type">date</span> <span class="type">DATE</span>,</span><br><span class="line">  eventId STRING,</span><br><span class="line">  eventType STRING,</span><br><span class="line">  data STRING)</span><br><span class="line"><span class="keyword">USING</span> DELTA</span><br><span class="line"><span class="comment">---------------------------------------------------------------------------------------------------------------------</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">OR</span> REPLACE <span class="keyword">TABLE</span> events (</span><br><span class="line">  <span class="type">date</span> <span class="type">DATE</span>,</span><br><span class="line">  eventId STRING,</span><br><span class="line">  eventType STRING,</span><br><span class="line">  data STRING)</span><br><span class="line"><span class="keyword">USING</span> DELTA</span><br></pre></td></tr></table></figure></li>
<li><p><strong><code>DataFrameWriter</code> API</strong>：如果想要创建表，同时将 Spark DataFrame 或数据集中的数据插入到该表中，则可以使用 Spark <code>DataFrameWriter</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create table in the metastore using DataFrame&#x27;s schema and write data to it</span></span><br><span class="line">df.write.format(<span class="string">&quot;delta&quot;</span>).saveAsTable(<span class="string">&quot;events&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create table with path using DataFrame&#x27;s schema and write data to it</span></span><br><span class="line">df.write.format(<span class="string">&quot;delta&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).save(<span class="string">&quot;/mnt/delta/events&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li><p><strong><code>DeltaTableBuilder</code> API</strong>：也可以使用 Delta Lake 中的 <code>DeltaTableBuilder</code> API 创建表。 与 DataFrameWriter API 相比，此 API 可以更轻松地指定其他信息，例如列注释、表属性和<a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/delta-batch#deltausegeneratedcolumns">生成的列 </a>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create table in the metastore</span></span><br><span class="line"><span class="type">DeltaTable</span>.createOrReplace(spark)</span><br><span class="line">  .tableName(<span class="string">&quot;event&quot;</span>)</span><br><span class="line">  .addColumn(<span class="string">&quot;date&quot;</span>, <span class="type">DateType</span>)</span><br><span class="line">  .addColumn(<span class="string">&quot;eventId&quot;</span>, <span class="string">&quot;STRING&quot;</span>)</span><br><span class="line">  .addColumn(<span class="string">&quot;eventType&quot;</span>, <span class="type">StringType</span>)</span><br><span class="line">  .addColumn(</span><br><span class="line">     <span class="type">DeltaTable</span>.columnBuilder(<span class="string">&quot;data&quot;</span>)</span><br><span class="line">       .dataType(<span class="string">&quot;STRING&quot;</span>)</span><br><span class="line">       .comment(<span class="string">&quot;event data&quot;</span>)</span><br><span class="line">       .build())</span><br><span class="line">  .execute()</span><br><span class="line">---------------------------------------------------------------------------------------------------------------------</span><br><span class="line"><span class="comment">// Create or replace table with path and add properties</span></span><br><span class="line"><span class="type">DeltaTable</span>.createOrReplace(spark)</span><br><span class="line">  .addColumn(<span class="string">&quot;date&quot;</span>, <span class="type">DateType</span>)</span><br><span class="line">  .addColumn(<span class="string">&quot;eventId&quot;</span>, <span class="string">&quot;STRING&quot;</span>)</span><br><span class="line">  .addColumn(<span class="string">&quot;eventType&quot;</span>, <span class="type">StringType</span>)</span><br><span class="line">  .addColumn(</span><br><span class="line">     <span class="type">DeltaTable</span>.columnBuilder(<span class="string">&quot;data&quot;</span>)</span><br><span class="line">       .dataType(<span class="string">&quot;STRING&quot;</span>)</span><br><span class="line">       .comment(<span class="string">&quot;event data&quot;</span>)</span><br><span class="line">       .build())</span><br><span class="line">  .location(<span class="string">&quot;/mnt/delta/events&quot;</span>)</span><br><span class="line">  .property(<span class="string">&quot;description&quot;</span>, <span class="string">&quot;table with event data&quot;</span>)</span><br><span class="line">  .execute()</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="2-1-2、创表时将数据分区"><a href="#2-1-2、创表时将数据分区" class="headerlink" title="2.1.2、创表时将数据分区"></a>2.1.2、创表时将数据分区</h5><p>你可以对数据进行分区，以加速其谓词涉及分区列的查询或 DML。 若要在创建 Delta 表时对数据进行分区，请指定按列分区。 常见的模式是按日期进行分区，例如：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SQL</span> ： </span><br><span class="line"><span class="comment">-- Create table in the metastore</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> events (</span><br><span class="line">  <span class="type">date</span> <span class="type">DATE</span>,</span><br><span class="line">  eventId STRING,</span><br><span class="line">  eventType STRING,</span><br><span class="line">  data STRING)</span><br><span class="line"><span class="keyword">USING</span> DELTA</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (<span class="type">date</span>)</span><br><span class="line"><span class="comment">---------------------------------------------------------------------------------------------------------------------</span></span><br><span class="line">Scala：</span><br><span class="line">df.write.format(&quot;delta&quot;).partitionBy(&quot;date&quot;).saveAsTable(&quot;events&quot;)</span><br><span class="line"><span class="comment">---------------------------------------------------------------------------------------------------------------------</span></span><br><span class="line">DeltaTable.createOrReplace(spark)</span><br><span class="line">  .tableName(&quot;event&quot;)</span><br><span class="line">  .addColumn(&quot;date&quot;, DateType)</span><br><span class="line">  .addColumn(&quot;eventId&quot;, &quot;STRING&quot;)</span><br><span class="line">  .addColumn(&quot;eventType&quot;, StringType)</span><br><span class="line">  .addColumn(&quot;data&quot;, &quot;STRING&quot;)</span><br><span class="line">  .partitionedBy(&quot;date&quot;)</span><br><span class="line">  .<span class="keyword">execute</span>()</span><br></pre></td></tr></table></figure>

<h5 id="2-1-3、空值数据位置"><a href="#2-1-3、空值数据位置" class="headerlink" title="2.1.3、空值数据位置"></a>2.1.3、空值数据位置</h5><blockquote>
<p>对于元存储中定义的表，可以选择性地将 <code>LOCATION</code> 指定为路径。 使用指定的 <code>LOCATION</code> 创建的表被视为不受元存储管理。 与不指定路径的托管表不同，非托管表的文件在你 <code>DROP</code> 表时不会被删除。</p>
<p>如果运行 <code>CREATE TABLE</code> 时指定的 <code>LOCATION</code> 已包含使用 Delta Lake 存储的数据，则 Delta Lake 会执行以下操作：</p>
<ul>
<li><p>如果仅指定了表名称和位置，例如：</p>
<p>SQL复制</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> events</span><br><span class="line"><span class="keyword">USING</span> DELTA</span><br><span class="line">LOCATION <span class="string">&#x27;/mnt/delta/events&#x27;</span></span><br></pre></td></tr></table></figure>

<p>元存储中的表会自动继承现有数据的架构、分区和表属性。 此功能可用于将数据“导入”到元存储中。</p>
</li>
<li><p>如果你指定了任何配置（架构、分区或表属性），则 Delta Lake 会验证指定的内容是否与现有数据的配置完全匹配。</p>
<p> <strong>重要</strong>：如果指定的配置与数据的配置并非完全匹配，则 Delta Lake 会引发一个描述差异的异常。</p>
</li>
</ul>
</blockquote>
<h4 id="2-2、读取表"><a href="#2-2、读取表" class="headerlink" title="2.2、读取表"></a>2.2、读取表</h4><blockquote>
<p>可以通过指定一个表名或路径将 Delta 表作为数据帧加载：</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SQL</span>:</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> events   <span class="comment">-- query table in the metastore</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> delta.`<span class="operator">/</span>mnt<span class="operator">/</span>delta<span class="operator">/</span>events`  <span class="comment">-- query table by path  </span></span><br><span class="line"><span class="comment">---------------------------------------------------------------------------------------------------------------------</span></span><br><span class="line">Scala:</span><br><span class="line">spark.table(&quot;events&quot;)      <span class="operator">/</span><span class="operator">/</span> query <span class="keyword">table</span> <span class="keyword">in</span> the metastore</span><br><span class="line">spark.read.format(&quot;delta&quot;).load(&quot;/mnt/delta/events&quot;)  <span class="operator">/</span><span class="operator">/</span> <span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">by</span> path</span><br><span class="line">import io.delta.implicits._</span><br><span class="line">spark.read.delta(&quot;/mnt/delta/events&quot;)</span><br></pre></td></tr></table></figure>

<p>数据保留</p>
<blockquote>
<p>若要按时间顺序查看以前的某个版本，必须同时保留该版本的日志文件和数据文件。</p>
<p>永远不会自动删除支持 Delta 表的数据文件；仅在运行 <a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/delta-utility#delta-vacuum">VACUUM</a> 时才会删除数据文件。 <code>VACUUM</code> 不会删除 Delta 日志文件；在写入检查点后自动清理日志文件。</p>
</blockquote>
<h4 id="2-3、写入到表"><a href="#2-3、写入到表" class="headerlink" title="2.3、写入到表"></a>2.3、写入到表</h4><ul>
<li><p>追加:要将新数据以原子方式添加到现有 Delta 表，请使用 <code>append</code> 模式：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SQL</span>: </span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> events <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> newEvents</span><br><span class="line"><span class="comment">---------------------------------------------------------------------------------------------------------------------</span></span><br><span class="line">scala :</span><br><span class="line">df.write.format(&quot;delta&quot;).mode(&quot;append&quot;).save(&quot;/mnt/delta/events&quot;)</span><br><span class="line">df.write.format(&quot;delta&quot;).mode(&quot;append&quot;).saveAsTable(&quot;events&quot;)</span><br><span class="line"><span class="comment">---------------------------------------------------------------------------------------------------------------------</span></span><br><span class="line">import io.delta.implicits._</span><br><span class="line">df.write.mode(&quot;append&quot;).delta(&quot;/mnt/delta/events&quot;)</span><br></pre></td></tr></table></figure></li>
<li><p>Overwrite : 要以原子方式替换表中的所有数据，请使用 <code>overwrite</code> 模式：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SQL</span>: </span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> events <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> newEvents</span><br><span class="line"><span class="comment">---------------------------------------------------------------------------------------------------------------------</span></span><br><span class="line">Scala:</span><br><span class="line">df.write.format(&quot;delta&quot;).mode(&quot;overwrite&quot;).save(&quot;/mnt/delta/events&quot;)</span><br><span class="line">df.write.format(&quot;delta&quot;).mode(&quot;overwrite&quot;).saveAsTable(&quot;events&quot;)</span><br><span class="line"></span><br><span class="line">import io.delta.implicits._</span><br><span class="line">df.write.mode(&quot;overwrite&quot;).delta(&quot;/mnt/delta/events&quot;)</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="2-4、更新表结构，替换表结构"><a href="#2-4、更新表结构，替换表结构" class="headerlink" title="2.4、更新表结构，替换表结构"></a>2.4、更新表结构，替换表结构</h4><ul>
<li><p>Delta Lake 允许你更新表的架构。 支持下列类型的更改：</p>
<ul>
<li>添加新列（在任意位置）</li>
<li>重新排列现有列</li>
</ul>
<p>你可以使用 DDL 显式地或使用 DML 隐式地进行这些更改。</p>
</li>
<li><p>默认情况下，覆盖表中的数据不会覆盖架构。 在不使用 <code>replaceWhere</code> 的情况下使用 <code>mode(&quot;overwrite&quot;)</code> 来覆盖表时，你可能还希望覆盖写入的数据的架构。 你可以通过将 <code>overwriteSchema</code> 选项设置为 <code>true</code> 来替换表的架构和分区：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.option(<span class="string">&quot;overwriteSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="2-5、表中的视图，表属性，表元数据"><a href="#2-5、表中的视图，表属性，表元数据" class="headerlink" title="2.5、表中的视图，表属性，表元数据"></a>2.5、表中的视图，表属性，表元数据</h4><ul>
<li><p>表中的视图：Delta Lake 支持基于 Delta 表创建视图，就像使用数据源表一样。</p>
<p>这些视图集成了<a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/security/access-control/table-acls/object-privileges">表访问控制</a>，可以实现列级和行级安全性。</p>
<p>处理视图时的主要难题是解析架构。 如果你更改 Delta 表架构，则必须重新创建派生视图，以容纳向该架构添加的任何内容。 例如，如果向 Delta 表中添加一个新列，则必须确保此列在基于该基表构建的相应视图中可用。</p>
</li>
<li><p>表属性：你可以使用 <code>CREATE</code> 和 <code>ALTER</code> 中的 <code>TBLPROPERTIES</code> 将自己的元数据存储为表属性。</p>
<p><code>TBLPROPERTIES</code> 存储为 Delta 表元数据的一部分。 如果在给定位置已存在 Delta 表，则无法在 <code>CREATE</code> 语句中定义新的 <code>TBLPROPERTIES</code>。 有关更多详细信息，请参阅<a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/delta-batch#ddlcreatetable">创建表</a>。</p>
<p>此外，为了调整行为和性能，Delta Lake 支持某些 Delta 表属性：</p>
<ul>
<li>阻止 Delta 表中的删除和更新：<code>delta.appendOnly=true</code>。</li>
<li>配置<a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/delta-batch#deltatimetravel">按时间顺序查看</a>保留属性：<code>delta.logRetentionDuration=&lt;interval-string&gt;</code> 和 <code>delta.deletedFileRetentionDuration=&lt;interval-string&gt;</code>。 有关详细信息，请参阅<a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/delta-batch#data-retention">数据保留</a>。</li>
<li>配置要为其收集统计信息的列的数目：<code>delta.dataSkippingNumIndexedCols=&lt;number-of-columns&gt;</code>。 此属性仅对写出的新数据有效。</li>
</ul>
<p> 备注</p>
<ul>
<li>修改 Delta 表属性是一个写入操作，该操作会与其他<a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/concurrency-control">并发写入操作</a>冲突，导致这些操作失败。 建议仅当不存在对表的并发写入操作时才修改表属性。</li>
</ul>
<p>你还可以在第一次提交到 Delta 表期间使用 Spark 配置来设置带 <code>delta.</code> 前缀的属性。 例如，若要使用属性 <code>delta.appendOnly=true</code> 初始化 Delta 表，请将 Spark 配置 <code>spark.databricks.delta.properties.defaults.appendOnly</code> 设置为 <code>true</code>。 例如： 。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SQL</span>：</span><br><span class="line">spark.sql(&quot;SET spark.databricks.delta.properties.defaults.appendOnly = true&quot;)</span><br><span class="line"></span><br><span class="line">Scala：</span><br><span class="line">spark.conf.set(&quot;spark.databricks.delta.properties.defaults.appendOnly&quot;, &quot;true&quot;)</span><br></pre></td></tr></table></figure></li>
<li><p>表元数据：Delta Lake 提供了丰富的用来浏览表元数据的功能。</p>
<ul>
<li>DESCRIBE DETAIL：提供架构、分区、表大小等方面的信息。</li>
<li>DESCRIBE HISTORY：提供出处信息，包括操作、用户等，以及向表的每次写入的操作指标。 表历史记录会保留 30 天。</li>
</ul>
</li>
</ul>
<h4 id="2-6、Delta-Lake-批处理命令笔记本（Demo）"><a href="#2-6、Delta-Lake-批处理命令笔记本（Demo）" class="headerlink" title="2.6、Delta Lake 批处理命令笔记本（Demo）"></a>2.6、Delta Lake 批处理命令笔记本（Demo）</h4><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/_static/notebooks/delta/quickstart-sql.html">获取笔记本</a></p>
<h3 id="3、表流读取和写入"><a href="#3、表流读取和写入" class="headerlink" title="3、表流读取和写入"></a>3、表流读取和写入</h3><blockquote>
<p>Delta Lake 通过 <code>readStream</code> 和 <code>writeStream</code> 与 <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html">Spark 结构化流式处理</a>深度集成。 Delta Lake 克服了通常与流式处理系统和文件相关的许多限制，包括：</p>
<ul>
<li>合并低延迟引入生成的小文件</li>
<li>保持对多个流（或并发批处理作业）执行“仅一次”处理</li>
<li>使用文件作为流源时，可以有效地发现哪些文件是新文件</li>
</ul>
</blockquote>
<h4 id="3-1-、用作源的-Delta-表"><a href="#3-1-、用作源的-Delta-表" class="headerlink" title="3.1 、用作源的 Delta 表"></a>3.1 、用作源的 Delta 表</h4><blockquote>
<p>将 Delta 表作为流源加载并在流式处理查询中使用它时，该查询将处理表中存在的所有数据以及流启动后到达的所有新数据。</p>
<p>可以将路径和表都作为流加载。</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">spark.readStream.format(<span class="string">&quot;delta&quot;</span>)</span><br><span class="line">  .load(<span class="string">&quot;/mnt/delta/events&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> io.delta.implicits._</span><br><span class="line">spark.readStream.delta(<span class="string">&quot;/mnt/delta/events&quot;</span>)</span><br><span class="line"></span><br><span class="line">或：--------------------------------------------------------------------------------------------------------------------- </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> io.delta.implicits._</span><br><span class="line"></span><br><span class="line">spark.readStream.format(<span class="string">&quot;delta&quot;</span>).table(<span class="string">&quot;events&quot;</span>)</span><br></pre></td></tr></table></figure>

<h5 id="3-1-1、限制输入速率"><a href="#3-1-1、限制输入速率" class="headerlink" title="3.1.1、限制输入速率"></a>3.1.1、限制输入速率</h5><blockquote>
<p>以下选项可用于控制微批处理：</p>
<ul>
<li><code>maxFilesPerTrigger</code>：每个微批处理中要考虑的新文件数。 默认值为 1000。</li>
<li><code>maxBytesPerTrigger</code>：每个微批处理中处理的数据量。 该选项会设置“软最大值”，即批处理大约可以处理这一数量的数据，并且可能处理超出该限制的数据量。 如果将 <code>Trigger.Once</code> 用于流式处理，此选项将被忽略。 默认情况下，未设置此项。</li>
</ul>
<p>如果将 <code>maxBytesPerTrigger</code> 与 <code>maxFilesPerTrigger</code> 结合使用，则微批处理将处理数据，直到达到 <code>maxFilesPerTrigger</code> 或 <code>maxBytesPerTrigger</code> 限制。</p>
<p> 备注: </p>
<p>如果源表事务由于 <code>logRetentionDuration</code> <a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/delta-batch#data-retention">配置</a>而被清除，并且流在处理中滞后，则 Delta Lake 会处理与源表的最新可用事务历史记录相对应的数据，但不会使流失败。 这可能会导致数据被丢弃。</p>
</blockquote>
<h5 id="3-1-2、忽略更新和删除"><a href="#3-1-2、忽略更新和删除" class="headerlink" title="3.1.2、忽略更新和删除"></a>3.1.2、忽略更新和删除</h5><blockquote>
<p>结构化流式处理不处理非追加的输入，并且会在对用作源的表进行了任何修改时引发异常。 可以通过两种主要策略处理无法自动向下游传播的更改：</p>
<ul>
<li>可以删除输出和检查点，并从头开始重启流。</li>
<li>可以设置以下两个选项之一：<ul>
<li><code>ignoreDeletes</code>：忽略在分区边界删除数据的事务。</li>
<li><code>ignoreChanges</code>：如果由于数据更改操作（例如 <code>UPDATE</code>、<code>MERGE INTO</code>、分区内的 <code>DELETE</code> 或 <code>OVERWRITE</code>）而不得不在源表中重写文件，则重新处理更新。 未更改的行仍可能发出，因此下游使用者应该能够处理重复项。 删除不会传播到下游。 <code>ignoreChanges</code> 包括 <code>ignoreDeletes</code>。 因此，如果使用 <code>ignoreChanges</code>，则流不会因源表的删除或更新而中断。</li>
</ul>
</li>
</ul>
</blockquote>
<p><strong>示例：</strong></p>
<p>例如，假设你有一个表 <code>user_events</code>，其中包含 <code>date</code>、<code>user_email</code> 和 <code>action</code> 列，并按 <code>date</code> 对该表进行了分区。 从 <code>user_events</code> 表向外进行流式处理，由于 GDPR 的原因，需要从中删除数据。</p>
<p>在分区边界（即 <code>WHERE</code> 位于分区列上）执行删除操作时，文件已经按值进行了分段，因此删除操作直接从元数据中删除这些文件。 因此，如果只想删除某些分区中的数据，则可以使用：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.readStream.format(<span class="string">&quot;delta&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;ignoreDeletes&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .load(<span class="string">&quot;/mnt/delta/user_events&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>但是，如果必须基于 <code>user_email</code> 删除数据，则需要使用：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.readStream.format(<span class="string">&quot;delta&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;ignoreChanges&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .load(<span class="string">&quot;/mnt/delta/user_events&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>如果使用 <code>UPDATE</code> 语句更新 <code>user_email</code>，则包含相关 <code>user_email</code> 的文件将被重写。 使用 <code>ignoreChanges</code> 时，新记录将与同一文件中的所有其他未更改记录一起传播到下游。 逻辑应该能够处理这些传入的重复记录。</p>
<h5 id="3-1-3、指定初始位置"><a href="#3-1-3、指定初始位置" class="headerlink" title="3.1.3、指定初始位置"></a>3.1.3、指定初始位置</h5><blockquote>
<p>此功能在 Databricks Runtime 7.3 LTS 及更高版本上可用。</p>
<p>可以使用以下选项来指定 Delta Lake 流式处理源的起点，而无需处理整个表。</p>
<ul>
<li><p><code>startingVersion</code>：要从其开始的 Delta Lake 版本。 从此版本（含）开始的所有表更改都将由流式处理源读取。 可以从 <a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/delta-utility#delta-history">DESCRIBE HISTORY</a> 命令输出的 <code>version</code> 列中获取提交版本。</p>
<p>若要仅返回 Databricks Runtime 7.4 及更高版本中的最新更改，请指定 <code>latest</code>。</p>
</li>
<li><p><code>startingTimestamp</code>：要从其开始的时间戳。 在该时间戳（含）或之后提交的所有表更改都将由流式处理源读取。 下列其中一项：</p>
<ul>
<li>时间戳字符串。 例如 <code>&quot;2019-01-01T00:00:00.000Z&quot;</code>。</li>
<li>日期字符串。 例如 <code>&quot;2019-01-01&quot;</code>。</li>
</ul>
</li>
</ul>
<p>不能同时设置这两个选项，只能使用其中一个选项。 这两个选项仅在启动新的流式处理查询时才生效。 如果流式处理查询已启动且已在其检查点中记录进度，这些选项将被忽略。</p>
<p> 重要:虽然可以从指定的版本或时间戳启动流式处理源，但流式处理源的架构始终是 Delta 表的最新架构。 必须确保在指定版本或时间戳之后，不对 Delta 表进行任何不兼容的架构更改。 否则，使用错误的架构读取数据时，流式处理源可能会返回不正确的结果。</p>
</blockquote>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">例如，假设你有一个表 user_events。 如果要从版本 <span class="number">5</span> 开始读取更改，请使用：</span><br><span class="line">spark.readStream.format(<span class="string">&quot;delta&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;startingVersion&quot;</span>, <span class="string">&quot;5&quot;</span>)</span><br><span class="line">  .load(<span class="string">&quot;/mnt/delta/user_events&quot;</span>)</span><br><span class="line"></span><br><span class="line">如果想了解自 <span class="number">2018</span> 年 <span class="number">10</span> 月 <span class="number">18</span> 日以来进行的更改，可使用：</span><br><span class="line">spark.readStream.format(<span class="string">&quot;delta&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;startingTimestamp&quot;</span>, <span class="string">&quot;2018-10-18&quot;</span>)</span><br><span class="line">  .load(<span class="string">&quot;/mnt/delta/user_events&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="3-2、用作接收器的Delta表"><a href="#3-2、用作接收器的Delta表" class="headerlink" title="3.2、用作接收器的Delta表"></a>3.2、用作接收器的Delta表</h4><p>你也可以使用结构化流式处理将数据写入 Delta 表。 即使有针对表并行运行的其他流或批处理查询，Delta Lake 也可通过事务日志确保“仅一次”处理。</p>
<h5 id="3-2-1、-追加模式"><a href="#3-2-1、-追加模式" class="headerlink" title="3.2.1、 追加模式"></a>3.2.1、 追加模式</h5><blockquote>
<p>默认情况下，流在追加模式下运行，这会将新记录添加到表中。</p>
</blockquote>
<ul>
<li><p>可以使用路径方法：    </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">events.writeStream</span><br><span class="line">  .format(<span class="string">&quot;delta&quot;</span>)</span><br><span class="line">  .outputMode(<span class="string">&quot;append&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;checkpointLocation&quot;</span>, <span class="string">&quot;/mnt/delta/events/_checkpoints/etl-from-json&quot;</span>)</span><br><span class="line">  .start(<span class="string">&quot;/mnt/delta/events&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> io.delta.implicits._</span><br><span class="line">events.writeStream</span><br><span class="line">  .outputMode(<span class="string">&quot;append&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;checkpointLocation&quot;</span>, <span class="string">&quot;/mnt/delta/events/_checkpoints/etl-from-json&quot;</span>)</span><br><span class="line">  .delta(<span class="string">&quot;/mnt/delta/events&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li><p>或表方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">events.writeStream</span><br><span class="line">  .outputMode(<span class="string">&quot;append&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;checkpointLocation&quot;</span>, <span class="string">&quot;/mnt/delta/events/_checkpoints/etl-from-json&quot;</span>)</span><br><span class="line">  .table(<span class="string">&quot;events&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="3-2-2、完整模式"><a href="#3-2-2、完整模式" class="headerlink" title="3.2.2、完整模式"></a>3.2.2、完整模式</h5><blockquote>
<p>你还可以使用结构化流式处理将整个表替换为每个批。 一个示例用例是使用聚合来计算摘要：</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">spark.readStream</span><br><span class="line">  .format(<span class="string">&quot;delta&quot;</span>)</span><br><span class="line">  .load(<span class="string">&quot;/mnt/delta/events&quot;</span>)</span><br><span class="line">  .groupBy(<span class="string">&quot;customerId&quot;</span>)</span><br><span class="line">  .count()</span><br><span class="line">  .writeStream</span><br><span class="line">  .format(<span class="string">&quot;delta&quot;</span>)</span><br><span class="line">  .outputMode(<span class="string">&quot;complete&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;checkpointLocation&quot;</span>, <span class="string">&quot;/mnt/delta/eventsByCustomer/_checkpoints/streaming-agg&quot;</span>)</span><br><span class="line">  .start(<span class="string">&quot;/mnt/delta/eventsByCustomer&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>上述示例持续更新包含按客户划分的事件总数的表。</p>
<p>对于延迟要求较为宽松的应用程序，可以使用一次性触发器来节省计算资源。 使用这些触发器按给定计划更新汇总聚合表，从而仅处理自上次更新以来收到的新数据。</p>
<h5 id="3-2-3、幂等多表写入"><a href="#3-2-3、幂等多表写入" class="headerlink" title="3.2.3、幂等多表写入"></a>3.2.3、幂等多表写入</h5><blockquote>
<p>备注:适用于 Databricks Runtime 8.4 及更高版本。</p>
<p>本部分介绍如何使用 <a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/spark/latest/structured-streaming/foreach">foreachBatch</a> 命令写入多个表。 通过<code>foreachBatch</code> 可将流查询中每个微批处理的输出写入多个目标。 但是，<code>foreachBatch</code> 不会使这些写入具有幂等性，因为这些写入尝试缺少批处理是否正在重新执行的信息。 例如，重新运行失败的批处理可能会导致重复的数据写入。</p>
<p>为了解决这个问题，Delta 表支持以下 DataFrameWriter 选项以使写入具有幂等性：</p>
<ul>
<li><code>txnAppId</code>：可以在每次 <code>DataFrame</code> 写入时传递的唯一字符串。 例如，可以使用 StreamingQuery ID 作为 <code>txnAppId</code>。</li>
<li><code>txnVersion</code>：充当事务版本的单调递增数字。</li>
</ul>
<p>Delta Lake 使用 <code>txnAppId</code> 和 <code>txnVersion</code> 的组合来识别重复写入并忽略它们。</p>
<p>如果批量写入因失败而中断，则重新运行该批次将使用相同的应用程序和批次 ID，这将有助于运行时正确识别重复写入并忽略它们。 应用程序 ID (<code>txnAppId</code>) 可以是任何用户生成的唯一字符串，不必与流 ID 相关。</p>
<p> <strong>警告</strong>:如果删除流式处理检查点并重启查询，则必须提供一个不同的 <code>appId</code>；否则，来自重启查询的写入将被忽略，因为它包含相同的 <code>txnAppId</code>，并且批处理 ID 从 0 开始。</p>
<p>举例： </p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">appId = ... <span class="comment">// A unique string that is used as application ID.</span></span><br><span class="line">streamingDF.writeStream.foreachBatch &#123; (batchDF: <span class="type">DataFrame</span>, batchId: <span class="type">Long</span>) =&gt;</span><br><span class="line">  batchDF.write.format(...).option(<span class="string">&quot;txnVersion&quot;</span>, batchId).option(<span class="string">&quot;txnAppId&quot;</span>, appId).save(...)  <span class="comment">// location 1</span></span><br><span class="line">  batchDF.write.format(...).option(<span class="string">&quot;txnVersion&quot;</span>, batchId).option(<span class="string">&quot;txnAppId&quot;</span>, appId).save(...)  <span class="comment">// location 2</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4、表删除、更新和合并"><a href="#4、表删除、更新和合并" class="headerlink" title="4、表删除、更新和合并"></a>4、表删除、更新和合并</h3><blockquote>
<p>Delta Lake 支持多个语句，以便在 Delta 表中删除数据和更新数据。</p>
</blockquote>
<h4 id="4-1、从表中删除"><a href="#4-1、从表中删除" class="headerlink" title="4.1、从表中删除"></a>4.1、从表中删除</h4><blockquote>
<p>可以从 Delta 表中删除与谓词匹配的数据。 例如，若要删除 <code>2017</code> 之前的所有事件，可以运行以下命令：</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SQL</span> : </span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> events <span class="keyword">WHERE</span> <span class="type">date</span> <span class="operator">&lt;</span> <span class="string">&#x27;2017-01-01&#x27;</span></span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> delta.`<span class="operator">/</span>data<span class="operator">/</span>events<span class="operator">/</span>` <span class="keyword">WHERE</span> <span class="type">date</span> <span class="operator">&lt;</span> <span class="string">&#x27;2017-01-01&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">---------------------------------------------------------------------------------------------------------------------</span></span><br><span class="line">Scala:可在 Databricks Runtime <span class="number">6.0</span> 及更高版本中使用 Scala API。</span><br><span class="line"></span><br><span class="line">import io.delta.tables._</span><br><span class="line">val deltaTable <span class="operator">=</span> DeltaTable.forPath(spark, &quot;/data/events/&quot;)</span><br><span class="line">deltaTable.delete(&quot;date &lt; &#x27;2017-01-01&#x27;&quot;)        <span class="operator">/</span><span class="operator">/</span> predicate <span class="keyword">using</span> <span class="keyword">SQL</span> formatted string</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.functions._</span><br><span class="line">import spark.implicits._</span><br><span class="line">deltaTable.delete(col(&quot;date&quot;) <span class="operator">&lt;</span> &quot;2017-01-01&quot;)       <span class="operator">/</span><span class="operator">/</span> predicate <span class="keyword">using</span> Spark <span class="keyword">SQL</span> functions <span class="keyword">and</span> implicits</span><br></pre></td></tr></table></figure>

<p>重要： <code>delete</code> 可以从最新版本的 Delta 表中删除数据，但是直到显式删除旧的版本后才能从物理存储中删除数据。</p>
<p>提示：如果可能，请在分区的 Delta 表的分区列上提供谓词，因为这样的谓词可以显著加快操作速度。</p>
<h4 id="4-2、更新表"><a href="#4-2、更新表" class="headerlink" title="4.2、更新表"></a>4.2、更新表</h4><blockquote>
<p>可以在 Delta 表中更新与谓词匹配的数据。 </p>
<p>例如，若要解决 <code>eventType</code> 中的拼写错误，可以运行以下命令：</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SQL</span>:</span><br><span class="line">UPDATE events <span class="keyword">SET</span> eventType <span class="operator">=</span> <span class="string">&#x27;click&#x27;</span> <span class="keyword">WHERE</span> eventType <span class="operator">=</span> <span class="string">&#x27;clck&#x27;</span></span><br><span class="line">UPDATE delta.`<span class="operator">/</span>data<span class="operator">/</span>events<span class="operator">/</span>` <span class="keyword">SET</span> eventType <span class="operator">=</span> <span class="string">&#x27;click&#x27;</span> <span class="keyword">WHERE</span> eventType <span class="operator">=</span> <span class="string">&#x27;clck&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">---------------------------------------------------------------------------------------------------------------------</span></span><br><span class="line">Scala:可在 Databricks Runtime <span class="number">6.0</span> 及更高版本中使用 Scala API。</span><br><span class="line">import io.delta.tables._</span><br><span class="line">val deltaTable <span class="operator">=</span> DeltaTable.forPath(spark, &quot;/data/events/&quot;)</span><br><span class="line">deltaTable.updateExpr(            <span class="operator">/</span><span class="operator">/</span> predicate <span class="keyword">and</span> update expressions <span class="keyword">using</span> <span class="keyword">SQL</span> formatted string</span><br><span class="line">  &quot;eventType = &#x27;clck&#x27;&quot;,</span><br><span class="line">  Map(&quot;eventType&quot; <span class="operator">-</span><span class="operator">&gt;</span> &quot;&#x27;click&#x27;&quot;)</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.functions._</span><br><span class="line">import spark.implicits._</span><br><span class="line">deltaTable.update(                <span class="operator">/</span><span class="operator">/</span> predicate <span class="keyword">using</span> Spark <span class="keyword">SQL</span> functions <span class="keyword">and</span> implicits</span><br><span class="line">  col(&quot;eventType&quot;) <span class="operator">=</span><span class="operator">=</span><span class="operator">=</span> &quot;clck&quot;,</span><br><span class="line">  Map(&quot;eventType&quot; <span class="operator">-</span><span class="operator">&gt;</span> lit(&quot;click&quot;)));</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>提示： 与删除类似，在分区上使用谓词可以显著提高更新操作的速度。</p>
<h4 id="4-3、使用合并操作在表中执行更新插入"><a href="#4-3、使用合并操作在表中执行更新插入" class="headerlink" title="4.3、使用合并操作在表中执行更新插入"></a>4.3、使用合并操作在表中执行更新插入</h4><blockquote>
<p>可以使用 <code>MERGE</code> SQL 操作将源表、视图或 DataFrame 中的数据更新插入到 Delta 表中。 Delta Lake 支持 <code>MERGE</code> 中的插入、更新和删除，并支持超出 SQL 标准的扩展语法以辅助高级用例。</p>
<p>假设你有一个 Spark DataFrame，它包含带有 <code>eventId</code> 的事件的新数据。 其中一些事件可能已经存在于 <code>events</code> 表中。 若要将新数据合并到 <code>events</code> 表中，你需要更新匹配的行（即，<code>eventId</code> 已经存在）并插入新行（即，<code>eventId</code> 不存在）。 可以运行以下查询：</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SQL</span>:</span><br><span class="line"></span><br><span class="line"><span class="keyword">MERGE</span> <span class="keyword">INTO</span> events</span><br><span class="line"><span class="keyword">USING</span> updates</span><br><span class="line"><span class="keyword">ON</span> events.eventId <span class="operator">=</span> updates.eventId</span><br><span class="line"><span class="keyword">WHEN</span> MATCHED <span class="keyword">THEN</span></span><br><span class="line">  UPDATE <span class="keyword">SET</span> events.data <span class="operator">=</span> updates.data</span><br><span class="line"><span class="keyword">WHEN</span> <span class="keyword">NOT</span> MATCHED <span class="keyword">THEN</span> </span><br><span class="line">  <span class="keyword">INSERT</span> (<span class="type">date</span>, eventId, data) <span class="keyword">VALUES</span> (<span class="type">date</span>, eventId, data)</span><br><span class="line"><span class="comment">---------------------------------------------------------------------------------------------------------------------</span></span><br><span class="line">Scala形式: </span><br><span class="line"></span><br><span class="line">import io.delta.tables._</span><br><span class="line">import org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">val updatesDF <span class="operator">=</span> ...  <span class="operator">/</span><span class="operator">/</span> <span class="keyword">define</span> the updates DataFrame[<span class="type">date</span>, eventId, data]</span><br><span class="line"></span><br><span class="line">DeltaTable.forPath(spark, &quot;/data/events/&quot;)</span><br><span class="line">  .<span class="keyword">as</span>(&quot;events&quot;)</span><br><span class="line">  .<span class="keyword">merge</span>(</span><br><span class="line">    updatesDF.as(&quot;updates&quot;),</span><br><span class="line">    &quot;events.eventId = updates.eventId&quot;)</span><br><span class="line">  .whenMatched   <span class="comment">-- 当匹配上，执行update</span></span><br><span class="line">  .updateExpr(</span><br><span class="line">    Map(&quot;data&quot; <span class="operator">-</span><span class="operator">&gt;</span> &quot;updates.data&quot;))</span><br><span class="line">  .whenNotMatched  <span class="comment">-- 当没匹配上，执行insert</span></span><br><span class="line">  .insertExpr(</span><br><span class="line">    Map(</span><br><span class="line">      &quot;date&quot; <span class="operator">-</span><span class="operator">&gt;</span> &quot;updates.date&quot;,</span><br><span class="line">      &quot;eventId&quot; <span class="operator">-</span><span class="operator">&gt;</span> &quot;updates.eventId&quot;,</span><br><span class="line">      &quot;data&quot; <span class="operator">-</span><span class="operator">&gt;</span> &quot;updates.data&quot;))</span><br><span class="line">  .<span class="keyword">execute</span>()</span><br></pre></td></tr></table></figure>

<p><strong>操作语义：</strong></p>
<p>下面是 <code>merge</code> 编程操作的详细说明。</p>
<ul>
<li><p>可以有任意数量的 <code>whenMatched</code> 和 <code>whenNotMatched</code> 子句。</p>
<p> 备注</p>
<p>在 Databricks Runtime 7.2 及更低版本中，<code>merge</code> 最多可以有 2 个 <code>whenMatched</code> 子句和最多 1 个 <code>whenNotMatched</code> 子句。</p>
</li>
<li><p>当源行根据匹配条件与目标表行匹配时，将执行 <code>whenMatched</code> 子句。 这些子句具有以下语义。</p>
<ul>
<li><p><code>whenMatched</code> 子句最多可以有 1 个 <code>update</code> 和 1 个 <code>delete</code> 操作。 <code>merge</code> 中的 <code>update</code> 操作只更新匹配目标行的指定列（类似于 <code>update</code> <a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/delta-update#delta-update">操作</a>）。 <code>delete</code> 操作删除匹配的行。</p>
</li>
<li><p>每个 <code>whenMatched</code> 子句都可以有一个可选条件。 如果存在此子句条件，则仅当该子句条件成立时，才对任何匹配的源-目标行对执行 <code>update</code> 或 <code>delete</code> 操作。</p>
</li>
<li><p>如果有多个 <code>whenMatched</code> 子句，则将按照指定的顺序对其进行求值（即，子句的顺序很重要）。 除最后一个之外，所有 <code>whenMatched</code> 子句都必须具有条件。</p>
</li>
<li><p>如果多个 <code>whenMatched</code> 子句都具有条件，并且对于匹配的源-目标行对都没有条件成立，那么匹配的目标行将保持不变。</p>
</li>
<li><p>若要使用源数据集的相应列更新目标 Delta 表的所有列，请使用 <code>whenMatched(...).updateAll()</code>。 这等效于：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">whenMatched(...).updateExpr(<span class="type">Map</span>(<span class="string">&quot;col1&quot;</span> -&gt; <span class="string">&quot;source.col1&quot;</span>, <span class="string">&quot;col2&quot;</span> -&gt; <span class="string">&quot;source.col2&quot;</span>, ...))</span><br></pre></td></tr></table></figure>

<p>针对目标 Delta 表的所有列。 因此，此操作假定源表的列与目标表的列相同，否则查询将引发分析错误。</p>
<p> 备注</p>
<p>启用自动架构迁移后，此行为将更改。 有关详细信息，请参阅<a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/delta-update#merge-schema-evolution">自动架构演变</a>。</p>
</li>
</ul>
</li>
<li><p>当源行根据匹配条件与任何目标行都不匹配时，将执行 <code>whenNotMatched</code> 子句。 这些子句具有以下语义。</p>
<ul>
<li><p><code>whenNotMatched</code> 子句只能具有 <code>insert</code> 操作。 新行是基于指定的列和相应的表达式生成的。 你无需指定目标表中的所有列。 对于未指定的目标列，将插入 <code>NULL</code>。</p>
<p> 备注</p>
<p>在 Databricks Runtime 6.5 及更低版本中，必须为 <code>INSERT</code> 操作提供目标表中的所有列。</p>
</li>
<li><p>每个 <code>whenNotMatched</code> 子句都可以有一个可选条件。 如果存在子句条件，则仅当源条件对该行成立时才插入该行。 否则，将忽略源列。</p>
</li>
<li><p>如果有多个 <code>whenNotMatched</code> 子句，则将按照指定的顺序对其进行求值（即，子句的顺序很重要）。 除最后一个之外，所有 <code>whenNotMatched</code> 子句都必须具有条件。</p>
</li>
<li><p>若要使用源数据集的相应列插入目标 Delta 表的所有列，请使用 <code>whenNotMatched(...).insertAll()</code>。 这等效于：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">whenNotMatched(...).insertExpr(<span class="type">Map</span>(<span class="string">&quot;col1&quot;</span> -&gt; <span class="string">&quot;source.col1&quot;</span>, <span class="string">&quot;col2&quot;</span> -&gt; <span class="string">&quot;source.col2&quot;</span>, ...))</span><br></pre></td></tr></table></figure>

<p>针对目标 Delta 表的所有列。 因此，此操作假定源表的列与目标表的列相同，否则查询将引发分析错误。</p>
<p> 备注:启用自动架构迁移后，此行为将更改。 有关详细信息，请参阅<a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/delta-update#merge-schema-evolution">自动架构演变</a>。</p>
</li>
</ul>
</li>
</ul>
<p> <strong>重要</strong></p>
<ul>
<li>如果源数据集的多行匹配，并且合并尝试更新目标 Delta 表的相同行，则 <code>merge</code> 操作可能会失败。 根据合并的 SQL 语义，这种更新操作模棱两可，因为尚不清楚应使用哪个源行来更新匹配的目标行。 你可以预处理源表来消除出现多个匹配项的可能性。 请参阅<a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/delta-update#write-change-data-into-a-delta-table">变更数据捕获示例</a> - 它显示如何对变更数据集（即源数据集）进行预处理，以仅保留每键的最新更改，然后再将更改应用到目标 Delta 表中。</li>
<li>如果源数据集是非确定性的，则merge操作可能会产生不正确的结果。 这是因为merge可能会对源数据集执行两次扫描，如果两次扫描产生的数据不同，则对表所做的最终更改可能不正确。 源中的非确定性可能以多种方式出现。 其中一些如下：<ul>
<li>从非 Delta 表读取。 例如，从 CSV 表中读取，其中多次扫描时的基础文件可能会有所不同。</li>
<li>使用非确定性操作。 例如，使用当前时间戳筛选数据的 <code>Dataset.filter()</code> 操作可以在多次扫描之间产生不同的结果。</li>
</ul>
</li>
<li>仅当视图已定义为 <code>CREATE VIEW viewName AS SELECT * FROM deltaTable</code> 时，才能对 SQL VIEW 应用 SQL <code>MERGE</code> 操作。</li>
</ul>
<p> 备注: 在 Databricks Runtime 7.3 LTS 及更高版本中，无条件删除匹配项时允许多个匹配项（因为即使有多个匹配项，无条件删除也非常明确）。</p>
<h5 id="4-3-1、性能调优"><a href="#4-3-1、性能调优" class="headerlink" title="4.3.1、性能调优"></a>4.3.1、<strong>性能调优</strong></h5><p>可以使用以下方法缩短合并所用时间：</p>
<ul>
<li><p><strong>减少匹配项的搜索空间</strong>：默认情况下，操作将搜索整个 <code>merge</code> Delta 表以查找源表中的匹配项。 加速 <code>merge</code> 的一种方法是通过在匹配条件中添加已知约束来缩小搜索范围。 例如，假设你有一个由 <code>country</code> 和 <code>date</code> 分区的表，并且你希望使用 <code>merge</code> 更新最后一天和特定国家/地区的信息。 添加条件</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">events.date <span class="operator">=</span> <span class="built_in">current_date</span>() <span class="keyword">AND</span> events.country <span class="operator">=</span> <span class="string">&#x27;USA&#x27;</span></span><br></pre></td></tr></table></figure>

<p>将使查询更快，因为它只在相关分区中查找匹配项。 此外，该方法还有助于减少与其他并发操作发生冲突的机会。 有关详细信息，请参阅<a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/concurrency-control">并发控制</a>。</p>
</li>
<li><p><strong>压缩文件</strong>：如果数据存储在许多小文件中，则读取数据以搜索匹配项可能会变慢。 可以将小文件压缩为更大的文件，以提高读取吞吐量。 有关详细信息，请参阅<a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/best-practices#compact-files">压缩文件</a>。</p>
</li>
<li><p><strong>控制写入的随机</strong> 分区：操作会多次混用数据， <code>merge</code> 以计算和写入更新的数据。 用于随机排列的任务的数量由 Spark 会话配置 <code>spark.sql.shuffle.partitions</code> 控制。 设置此参数不仅可以控制并行度，还可以确定输出文件的数量。 增大该值可提高并行度，但也会生成大量较小的数据文件。</p>
</li>
<li><p><strong>启用优化写入</strong>：对于已分区表，<code>merge</code> 生成的小文件数量远大于随机分区的数量。 这是因为每个随机任务都可以在多个分区中写入多个文件，并可能成为性能瓶颈。 可以通过启用<a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/optimizations/auto-optimize#how-auto-optimize-works">优化写入</a>来减少文件数量。</p>
</li>
</ul>
<p> 备注</p>
<p>在 Databricks Runtime 7.4 及更高版本中，在对分区表进行 <code>merge</code> 操作时，会自动启用<a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/optimizations/auto-optimize#how-auto-optimize-works">优化写入</a>。</p>
<ul>
<li><strong>调整表中的文件大小</strong>：自 &lt;DBR 8.2&gt; 起，Azure Databricks 可以自动检测增量表是否在频繁执行重写文件的 <code>merge</code> 操作，并可能会减小重写文件的大小，以备将来执行更多文件重写操作。 有关详细信息，请参阅有关<a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/delta/optimizations/file-mgmt#autotune-based-on-table-size">调整文件大小</a>的部分。</li>
</ul>
<h4 id="4-4、合并操作示例"><a href="#4-4、合并操作示例" class="headerlink" title="4.4、合并操作示例"></a>4.4、合并操作示例</h4><h5 id="4-4-1、写入-Delta-表时进行重复数据删除"><a href="#4-4-1、写入-Delta-表时进行重复数据删除" class="headerlink" title="4.4.1、写入 Delta 表时进行重复数据删除"></a>4.4.1、写入 Delta 表时进行重复数据删除</h5><blockquote>
<p>一个常见的 ETL 用例是通过将日志附加到表中来将其收集到 Delta 表中。 但是，源通常可以生成重复的日志记录，因此需要下游重复数据删除步骤来处理它们。 通过 <code>merge</code>，你可以避免插入重复记录。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SQL</span> :</span><br><span class="line"><span class="keyword">MERGE</span> <span class="keyword">INTO</span> logs</span><br><span class="line"><span class="keyword">USING</span> newDedupedLogs</span><br><span class="line"><span class="keyword">ON</span> logs.uniqueId <span class="operator">=</span> newDedupedLogs.uniqueId</span><br><span class="line"><span class="keyword">WHEN</span> <span class="keyword">NOT</span> MATCHED</span><br><span class="line">  <span class="keyword">THEN</span> <span class="keyword">INSERT</span> <span class="operator">*</span></span><br><span class="line">  </span><br><span class="line">Scala： <span class="comment">-------------------------------------------------------------------------------------- </span></span><br><span class="line">  deltaTable</span><br><span class="line">  .<span class="keyword">as</span>(&quot;logs&quot;)</span><br><span class="line">  .<span class="keyword">merge</span>(</span><br><span class="line">    newDedupedLogs.as(&quot;newDedupedLogs&quot;),</span><br><span class="line">    &quot;logs.uniqueId = newDedupedLogs.uniqueId&quot;)</span><br><span class="line">  .whenNotMatched()</span><br><span class="line">  .insertAll()</span><br><span class="line">  .<span class="keyword">execute</span>()</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>如果你知道几天之内可能会得到重复记录，则可以通过按日期对表进行分区，然后指定要匹配的目标表的日期范围来进一步优化查询。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SQL</span>:</span><br><span class="line"><span class="keyword">MERGE</span> <span class="keyword">INTO</span> logs</span><br><span class="line"><span class="keyword">USING</span> newDedupedLogs</span><br><span class="line"><span class="keyword">ON</span> logs.uniqueId <span class="operator">=</span> newDedupedLogs.uniqueId <span class="keyword">AND</span> logs.date <span class="operator">&gt;</span> <span class="built_in">current_date</span>() <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="number">7</span> DAYS</span><br><span class="line"><span class="keyword">WHEN</span> <span class="keyword">NOT</span> MATCHED <span class="keyword">AND</span> newDedupedLogs.date <span class="operator">&gt;</span> <span class="built_in">current_date</span>() <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="number">7</span> DAYS</span><br><span class="line">  <span class="keyword">THEN</span> <span class="keyword">INSERT</span> <span class="operator">*</span></span><br><span class="line">  </span><br><span class="line">Scala： <span class="comment">-------------------------------------------------------------------------------------- </span></span><br><span class="line">deltaTable.as(&quot;logs&quot;).<span class="keyword">merge</span>(</span><br><span class="line">    newDedupedLogs.as(&quot;newDedupedLogs&quot;),</span><br><span class="line">    &quot;logs.uniqueId = newDedupedLogs.uniqueId AND logs.date &gt; current_date() - INTERVAL 7 DAYS&quot;)</span><br><span class="line">  .whenNotMatched(&quot;newDedupedLogs.date &gt; current_date() - INTERVAL 7 DAYS&quot;)</span><br><span class="line">  .insertAll()</span><br><span class="line">  .<span class="keyword">execute</span>()</span><br></pre></td></tr></table></figure>
</blockquote>
<h5 id="4-4-2、使用-从流式处理查询更新插入foreachBatch"><a href="#4-4-2、使用-从流式处理查询更新插入foreachBatch" class="headerlink" title="4.4.2、使用 从流式处理查询更新插入foreachBatch"></a>4.4.2、使用 从流式处理查询更新插入foreachBatch</h5><blockquote>
<p>可以使用 <code>merge</code> 和 <code>foreachBatch</code> 的组合（有关详细信息，请参阅 <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#foreachbatch">foreachbatch</a>）将复杂的更新插入操作从流式处理查询写入 Delta 表。 例如： 。</p>
<ul>
<li><strong>在更新模式下写入流式处理聚合</strong>：这比完整模式高效得多。</li>
<li><strong>将数据库更改流写入 Delta</strong> 表：用于写入 更改数据的合并查询可用于持续将更改 <code>foreachBatch</code> 流应用于 Delta 表。</li>
<li>使用重复数据删除将流数据写入 <strong>Delta</strong> 表：在 中，重复数据删除的仅插入合并查询可用于将数据 (和重复) 写入到具有自动重复数据删除的 <code>foreachBatch</code> Delta 表。</li>
</ul>
<p> 备注</p>
<ul>
<li>请确保 <code>foreachBatch</code> 中的 <code>merge</code> 语句是幂等的，因为重启流式处理查询可以将操作多次应用于同一批数据。</li>
<li>在 <code>foreachBatch</code> 中使用 <code>merge</code> 时，流式处理查询的输入数据速率（通过 <code>StreamingQueryProgress</code> 报告并在笔记本计算机速率图中可见）可以报告为源处生成数据的实际速率的倍数。 这是因为 <code>merge</code> 多次读取输入数据，导致输入指标倍增。 如果这是一个瓶颈，则可以在 <code>merge</code> 之前缓存批处理 DataFrame，然后在 <code>merge</code> 之后取消缓存。</li>
</ul>
</blockquote>
<p>使用 merge 和 foreachBatch 笔记本在更新模式下写入流式处理聚合(demo) <a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/azure/databricks/_static/notebooks/merge-in-streaming.html">获取笔记本</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">我不是铁杆</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://wobushitiegan.github.io/2021/09/28/study/databricks/">https://wobushitiegan.github.io/2021/09/28/study/databricks/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://wobushitiegan.github.io" target="_blank">wobushitiegan</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/09/28/hexo/error/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">hexo错误集锦</div></div></a></div><div class="next-post pull-right"><a href="/2021/09/28/hello-world/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Hello World</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1guwp9xw09sj60b00b2q3u02.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">我不是铁杆</div><div class="author-info__description">erwa的个人博客</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">4</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://wobushitiegan.top/"><i class="fas fa-child"></i><span>个人博客</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/wobushitiegan" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/wobushitiegan@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81Delta-Lake-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8"><span class="toc-text">一、Delta Lake 快速入门</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E5%88%9B%E5%BB%BA%E8%A1%A8"><span class="toc-text">1、创建表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E5%B0%86%E6%95%B0%E6%8D%AE%E5%88%86%E5%8C%BA"><span class="toc-text">2、将数据分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E4%BF%AE%E6%94%B9%E8%A1%A8"><span class="toc-text">3、修改表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E8%AF%BB%E5%8F%96%E8%A1%A8"><span class="toc-text">4、读取表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E3%80%81%E6%98%BE%E7%A4%BA%E8%A1%A8%E5%8E%86%E5%8F%B2%E8%AE%B0%E5%BD%95"><span class="toc-text">5、显示表历史记录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6%E3%80%81%E6%9F%A5%E8%AF%A2%E8%BE%83%E6%97%A9%E7%89%88%E6%9C%AC%E7%9A%84%E8%A1%A8%EF%BC%88%E6%8C%89%E6%97%B6%E9%97%B4%E9%A1%BA%E5%BA%8F%E6%9F%A5%E7%9C%8B%EF%BC%89"><span class="toc-text">6、查询较早版本的表（按时间顺序查看）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7%E3%80%81%E4%BC%98%E5%8C%96%E8%A1%A8"><span class="toc-text">7、优化表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8%E3%80%81%E6%8C%89%E5%88%97%E8%BF%9B%E8%A1%8CZ%E6%8E%92%E5%BA%8F"><span class="toc-text">8、按列进行Z排序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9%E3%80%81%E4%BB%8B%E7%BB%8D%E6%80%A7%E7%AC%94%E8%AE%B0%E6%9C%AC%EF%BC%88Demo%EF%BC%89"><span class="toc-text">9、介绍性笔记本（Demo）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%95%B0%E6%8D%AE%E7%9A%84%E8%AF%BB%E5%8F%96%E4%B8%8E%E5%86%99%E5%85%A5"><span class="toc-text">二、数据的读取与写入</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E5%B0%86%E6%95%B0%E6%8D%AE%E5%BC%95%E5%85%A5Delta-Lake"><span class="toc-text">1、将数据引入Delta Lake</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E8%A1%A8%E6%89%B9%E9%87%8F%E8%AF%BB%E5%8F%96%E5%92%8C%E5%86%99%E5%85%A5"><span class="toc-text">2、表批量读取和写入</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1%E3%80%81-%E5%88%9B%E5%BB%BA%E8%A1%A8"><span class="toc-text">2.1、 创建表</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-1%E3%80%81%E5%88%9B%E5%BB%BA%E8%A1%A8%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F%E3%80%82"><span class="toc-text">2.1.1、创建表的三种方式。</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-2%E3%80%81%E5%88%9B%E8%A1%A8%E6%97%B6%E5%B0%86%E6%95%B0%E6%8D%AE%E5%88%86%E5%8C%BA"><span class="toc-text">2.1.2、创表时将数据分区</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-3%E3%80%81%E7%A9%BA%E5%80%BC%E6%95%B0%E6%8D%AE%E4%BD%8D%E7%BD%AE"><span class="toc-text">2.1.3、空值数据位置</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2%E3%80%81%E8%AF%BB%E5%8F%96%E8%A1%A8"><span class="toc-text">2.2、读取表</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3%E3%80%81%E5%86%99%E5%85%A5%E5%88%B0%E8%A1%A8"><span class="toc-text">2.3、写入到表</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4%E3%80%81%E6%9B%B4%E6%96%B0%E8%A1%A8%E7%BB%93%E6%9E%84%EF%BC%8C%E6%9B%BF%E6%8D%A2%E8%A1%A8%E7%BB%93%E6%9E%84"><span class="toc-text">2.4、更新表结构，替换表结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5%E3%80%81%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%A7%86%E5%9B%BE%EF%BC%8C%E8%A1%A8%E5%B1%9E%E6%80%A7%EF%BC%8C%E8%A1%A8%E5%85%83%E6%95%B0%E6%8D%AE"><span class="toc-text">2.5、表中的视图，表属性，表元数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-6%E3%80%81Delta-Lake-%E6%89%B9%E5%A4%84%E7%90%86%E5%91%BD%E4%BB%A4%E7%AC%94%E8%AE%B0%E6%9C%AC%EF%BC%88Demo%EF%BC%89"><span class="toc-text">2.6、Delta Lake 批处理命令笔记本（Demo）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E8%A1%A8%E6%B5%81%E8%AF%BB%E5%8F%96%E5%92%8C%E5%86%99%E5%85%A5"><span class="toc-text">3、表流读取和写入</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-%E3%80%81%E7%94%A8%E4%BD%9C%E6%BA%90%E7%9A%84-Delta-%E8%A1%A8"><span class="toc-text">3.1 、用作源的 Delta 表</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#3-1-1%E3%80%81%E9%99%90%E5%88%B6%E8%BE%93%E5%85%A5%E9%80%9F%E7%8E%87"><span class="toc-text">3.1.1、限制输入速率</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-1-2%E3%80%81%E5%BF%BD%E7%95%A5%E6%9B%B4%E6%96%B0%E5%92%8C%E5%88%A0%E9%99%A4"><span class="toc-text">3.1.2、忽略更新和删除</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-1-3%E3%80%81%E6%8C%87%E5%AE%9A%E5%88%9D%E5%A7%8B%E4%BD%8D%E7%BD%AE"><span class="toc-text">3.1.3、指定初始位置</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2%E3%80%81%E7%94%A8%E4%BD%9C%E6%8E%A5%E6%94%B6%E5%99%A8%E7%9A%84Delta%E8%A1%A8"><span class="toc-text">3.2、用作接收器的Delta表</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-1%E3%80%81-%E8%BF%BD%E5%8A%A0%E6%A8%A1%E5%BC%8F"><span class="toc-text">3.2.1、 追加模式</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-2%E3%80%81%E5%AE%8C%E6%95%B4%E6%A8%A1%E5%BC%8F"><span class="toc-text">3.2.2、完整模式</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-3%E3%80%81%E5%B9%82%E7%AD%89%E5%A4%9A%E8%A1%A8%E5%86%99%E5%85%A5"><span class="toc-text">3.2.3、幂等多表写入</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E8%A1%A8%E5%88%A0%E9%99%A4%E3%80%81%E6%9B%B4%E6%96%B0%E5%92%8C%E5%90%88%E5%B9%B6"><span class="toc-text">4、表删除、更新和合并</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1%E3%80%81%E4%BB%8E%E8%A1%A8%E4%B8%AD%E5%88%A0%E9%99%A4"><span class="toc-text">4.1、从表中删除</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2%E3%80%81%E6%9B%B4%E6%96%B0%E8%A1%A8"><span class="toc-text">4.2、更新表</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3%E3%80%81%E4%BD%BF%E7%94%A8%E5%90%88%E5%B9%B6%E6%93%8D%E4%BD%9C%E5%9C%A8%E8%A1%A8%E4%B8%AD%E6%89%A7%E8%A1%8C%E6%9B%B4%E6%96%B0%E6%8F%92%E5%85%A5"><span class="toc-text">4.3、使用合并操作在表中执行更新插入</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#4-3-1%E3%80%81%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98"><span class="toc-text">4.3.1、性能调优</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4%E3%80%81%E5%90%88%E5%B9%B6%E6%93%8D%E4%BD%9C%E7%A4%BA%E4%BE%8B"><span class="toc-text">4.4、合并操作示例</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#4-4-1%E3%80%81%E5%86%99%E5%85%A5-Delta-%E8%A1%A8%E6%97%B6%E8%BF%9B%E8%A1%8C%E9%87%8D%E5%A4%8D%E6%95%B0%E6%8D%AE%E5%88%A0%E9%99%A4"><span class="toc-text">4.4.1、写入 Delta 表时进行重复数据删除</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-4-2%E3%80%81%E4%BD%BF%E7%94%A8-%E4%BB%8E%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%E6%9F%A5%E8%AF%A2%E6%9B%B4%E6%96%B0%E6%8F%92%E5%85%A5foreachBatch"><span class="toc-text">4.4.2、使用 从流式处理查询更新插入foreachBatch</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/09/28/hexo/error/" title="hexo错误集锦"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="hexo错误集锦"/></a><div class="content"><a class="title" href="/2021/09/28/hexo/error/" title="hexo错误集锦">hexo错误集锦</a><time datetime="2021-09-28T11:15:59.974Z" title="发表于 2021-09-28 19:15:59">2021-09-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/09/28/study/databricks/" title="Databricks"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Databricks"/></a><div class="content"><a class="title" href="/2021/09/28/study/databricks/" title="Databricks">Databricks</a><time datetime="2021-09-28T11:11:06.456Z" title="发表于 2021-09-28 19:11:06">2021-09-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/09/28/hello-world/" title="Hello World"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hello World"/></a><div class="content"><a class="title" href="/2021/09/28/hello-world/" title="Hello World">Hello World</a><time datetime="2021-09-28T11:11:06.455Z" title="发表于 2021-09-28 19:11:06">2021-09-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/09/27/mylife/%E4%BB%8A%E5%A4%A9-%E6%88%91%E7%9A%84%E7%94%9F%E6%97%A5/" title="今天,我的生日"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="今天,我的生日"/></a><div class="content"><a class="title" href="/2021/09/27/mylife/%E4%BB%8A%E5%A4%A9-%E6%88%91%E7%9A%84%E7%94%9F%E6%97%A5/" title="今天,我的生日">今天,我的生日</a><time datetime="2021-09-27T12:40:04.000Z" title="发表于 2021-09-27 20:40:04">2021-09-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2021 By 我不是铁杆</div><div class="framework-info"><span>framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><a href="a"><img class="icp-icon" src="/images/icp.png"><span>备案号：备案中</span></a><p><span>Hosted by <a target="_blank" rel="noopener" href="https://pages.coding.me" style="font-weight:bold">Coding Pages</a></span> and <span><a target="_blank" rel="noopener" href="https://github.com" style="font-weight:bold">Github</a></span></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'H6oC3nqGaKTWDftstJ8LHrYx-gzGzoHsz',
      appKey: 'tykicouAnzTBlK4VFTHdwUtd',
      placeholder: '记得留下你的昵称和邮箱....可以快速收到回复',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: true,
      serverURLs: 'wobushitiegan.top',
      emojiCDN: '//i0.hdslb.com/bfs/emote/',
      emojiMaps: {"tv_doge":"6ea59c827c414b4a2955fe79e0f6fd3dcd515e24.png","tv_亲亲":"a8111ad55953ef5e3be3327ef94eb4a39d535d06.png","tv_偷笑":"bb690d4107620f1c15cff29509db529a73aee261.png","tv_再见":"180129b8ea851044ce71caf55cc8ce44bd4a4fc8.png","tv_冷漠":"b9cbc755c2b3ee43be07ca13de84e5b699a3f101.png","tv_发怒":"34ba3cd204d5b05fec70ce08fa9fa0dd612409ff.png","tv_发财":"34db290afd2963723c6eb3c4560667db7253a21a.png","tv_可爱":"9e55fd9b500ac4b96613539f1ce2f9499e314ed9.png","tv_吐血":"09dd16a7aa59b77baa1155d47484409624470c77.png","tv_呆":"fe1179ebaa191569b0d31cecafe7a2cd1c951c9d.png","tv_呕吐":"9f996894a39e282ccf5e66856af49483f81870f3.png","tv_困":"241ee304e44c0af029adceb294399391e4737ef2.png","tv_坏笑":"1f0b87f731a671079842116e0991c91c2c88645a.png","tv_大佬":"093c1e2c490161aca397afc45573c877cdead616.png","tv_大哭":"23269aeb35f99daee28dda129676f6e9ea87934f.png","tv_委屈":"d04dba7b5465779e9755d2ab6f0a897b9b33bb77.png","tv_害羞":"a37683fb5642fa3ddfc7f4e5525fd13e42a2bdb1.png","tv_尴尬":"7cfa62dafc59798a3d3fb262d421eeeff166cfa4.png","tv_微笑":"70dc5c7b56f93eb61bddba11e28fb1d18fddcd4c.png","tv_思考":"90cf159733e558137ed20aa04d09964436f618a1.png","tv_惊吓":"0d15c7e2ee58e935adc6a7193ee042388adc22af.png"},
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>